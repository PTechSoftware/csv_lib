use std::sync::{Arc, Mutex};
use crate::models::csv_config::CsvConfig;
use crate::models::row::Row;
use crate::models::worker_status::WorkerResult;
use crate::models::workers::Worker;

pub struct MasterParallel<'mmap, F, T>
where
    F: FnMut(&mut Row<'mmap>, &CsvConfig, &mut T) + Send + Clone + 'mmap,
    T: Send + 'mmap,
{
    execution : F,
    target : &'mmap mut Mutex<T>,
    file: &'mmap [u8],
    cfg: &'mmap CsvConfig

}


impl<'mmap, F, T> MasterParallel<'mmap, F, T>
where
    F: FnMut(&mut Row<'mmap>, &CsvConfig, &mut T) + Send+ Clone + 'mmap,
    T: Send + 'mmap,
{

    /// ## Constructor
    /// Creates a new master instance.
    ///
    /// ## Arguments:
    /// - `file`: A reference of the slice generated by Mmap
    /// - `execution` : A closure with the code to inject in each row process
    /// - `target` : A generic struct that you can use to acumulate data(be sure is inside an Arc/Mutex)
    pub fn new(
        file: &'mmap [u8],
        cfg: &'mmap CsvConfig,
        execution: F,
        target: &'mmap mut Mutex<T>,

    ) -> Self {
        Self {
            execution,
            target,
            file,
            cfg
        }
    }
    /// Get available number of cores
    fn get_cores() -> usize {
        std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(4)
    }

    /// ## Runner One Thread
    ///
    /// - Processes assigned lines with the provided closure.
    /// - Is the Async version of the library, processing is sequential, and only uses one core.
    pub async fn run_parallel_one_thread<'a>(&'a mut self) -> WorkerResult
    where
        'a: 'mmap
    {
        let closure: F = self.execution.clone();
        let mut w = Worker::new(
            self.file,
            self.cfg,
            0,
            self.file.len(),
            closure,
            self.target,
        );
        w.runner_row().await
    }

    pub fn run_parallel_by_lines(&mut self) -> WorkerResult
    where
        T: Send,
        F: FnMut(&mut Row<'mmap>, &CsvConfig, &mut T) + Send + Clone,
    {
        use crossbeam::thread;

        let num_threads = Self::get_cores();
        let line_break = self.cfg.line_break;
        let file = self.file;
        let cfg = self.cfg;
        let total_len = file.len();
        let mut line_indices = vec![0];

        // Encontrar los offsets de cada línea [cambiar] 
        for (i, b) in file.iter().enumerate() {
            if *b == line_break {
                line_indices.push(i + 1);
            }
        }

        // Dividir líneas de forma equitativa
        let chunk_size = (line_indices.len() + num_threads - 1) / num_threads;
        let chunks: Vec<_> = line_indices
            .chunks(chunk_size)
            .filter_map(|chunk| {
                if let (Some(&start), Some(&end)) = (chunk.first(), chunk.last()) {
                    Some((start, end))
                } else {
                    None
                }
            })
            .collect();

        
        // Paralelizar
        let result = thread::scope(|s| {
            for (start, end) in chunks {
                let slice = &file[start..end];
                let mut closure = self.execution.clone();
                let cfg = cfg;
                let target = Arc::new(self.target);

                s.spawn(move |_| {
                    let mut worker = Worker::new(slice, cfg, 0, slice.len(), closure, target);
                    let _ = worker.runner_row(); // ignoramos errores por ahora
                });
            }
        }).map_err(|_| WorkerResult::Err("".to_string()));
        
        match result {
            Ok(_) => {
                WorkerResult::Ok
            }
            Err(er) => {
                er
            }
        }
    }






}